{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical example\n",
    "### Consider the following problem over $\\Omega = {(-1, 1)}^d$: \n",
    "### $$-\\Delta u + \\pi^2 u = f,$$\n",
    "### with periodic condition\n",
    "### $$ u(x_1 + p_1,\\cdots,x_{k} + p_k,\\cdots,x_d + p_d) = u(x_1,\\cdots,x_{k},\\cdots,x_d) $$ \n",
    "### Assume $u(x) = \\sum_{i = 1}^d \\cos(\\pi x_i) + \\cos(2 \\pi x_i) $, we can get $f(x)$ and $p_1 = \\cdots = p_d = 2$.\n",
    "### Network structure\n",
    "### construct a transform $x^{\\prime} = \\text{transform} (x)$ before the first fully connected layer of our neural network\n",
    "### $$x = (x_1,\\cdots,x_d) \\in R^d \\Rightarrow x^{\\prime} \\in R^{2d}$$\n",
    "### where $x^{\\prime}_{2i - 1} = \\sin(2\\pi x_i / p_i)$ and $x^{\\prime}_{2i} = \\cos(2\\pi x_i / p_i)$ for $i = 1, 2, \\cdots, d$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR, MultiStepLR\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import *\n",
    "import time\n",
    "# torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type('torch.DoubleTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation function\n",
    "def activation(x):\n",
    "    return x * torch.sigmoid(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(x):\n",
    "    x_transform = torch.zeros(len(x), 2*d)\n",
    "    for index in range(d):\n",
    "        x_transform[:, 2*index] = torch.sin(2*pi*x[:, index] / 2)\n",
    "        x_transform[:, 2*index+1] = torch.cos(2*pi*x[:, index] / 2)\n",
    "    return x_transform.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exact solution\n",
    "def u_ex(x):  \n",
    "#     x_temp_1 = torch.cos(pi*x)\n",
    "#     u_temp_1 = (x_temp_1.sum(1)).reshape([x.size()[0], 1]) # x_temp.sum(1) # summation by row for x_temp\n",
    "#     x_temp_2 = torch.cos(2*pi*x)\n",
    "#     u_temp_2 = (x_temp_2.sum(1)).reshape([x.size()[0], 1]) \n",
    "#     x_temp_3 = torch.cos(4*pi*x)\n",
    "#     u_temp_3 = (x_temp_3.sum(1)).reshape([x.size()[0], 1]) \n",
    "#     x_temp_4 = torch.cos(8*pi*x)\n",
    "#     u_temp_4 = (x_temp_4.sum(1)).reshape([x.size()[0], 1]) \n",
    "#     u_temp = u_temp_1 + u_temp_2 + u_temp_3 + u_temp_4\n",
    "    \n",
    "    x_temp_1 = torch.cos(pi*x)\n",
    "    u_temp_1 = (x_temp_1.sum(1)).reshape([x.size()[0], 1]) # x_temp.sum(1) # summation by row for x_temp\n",
    "    x_temp_2 = torch.cos(2*pi*x)\n",
    "    u_temp_2 = (x_temp_2.sum(1)).reshape([x.size()[0], 1]) \n",
    "   \n",
    "    u_temp = u_temp_1 + u_temp_2 \n",
    "    return u_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "#     x_temp_1 = torch.cos(pi*x)\n",
    "#     f_temp_1 = pi**2 * (x_temp_1.sum(1)).reshape([x.size()[0], 1]) \n",
    "#     x_temp_2 = torch.cos(2*pi*x)\n",
    "#     f_temp_2 = (2*pi)**2 * (x_temp_2.sum(1)).reshape([x.size()[0], 1]) \n",
    "#     x_temp_3 = torch.cos(4*pi*x)\n",
    "#     f_temp_3 = (4*pi)**2 * (x_temp_3.sum(1)).reshape([x.size()[0], 1]) \n",
    "#     x_temp_4 = torch.cos(8*pi*x)\n",
    "#     f_temp_4 = (8*pi)**2 * (x_temp_4.sum(1)).reshape([x.size()[0], 1]) \n",
    "#     f_temp = f_temp_1 + f_temp_2 + f_temp_3 + f_temp_4 + pi**2 * u_ex(x)\n",
    "    \n",
    "    x_temp_1 = torch.cos(pi*x)\n",
    "    f_temp_1 = pi**2 * (x_temp_1.sum(1)).reshape([x.size()[0], 1]) \n",
    "    x_temp_2 = torch.cos(2*pi*x)\n",
    "    f_temp_2 = (2*pi)**2 * (x_temp_2.sum(1)).reshape([x.size()[0], 1]) \n",
    "    f_temp = f_temp_1 + f_temp_2 + pi**2 * u_ex(x)\n",
    "    return f_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build ResNet with three blocks\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,input_size,width,output_size):\n",
    "        super(Net,self).__init__()\n",
    "        self.layer_in = nn.Linear(input_size,width)\n",
    "        self.layer_1 = nn.Linear(width,width)\n",
    "        self.layer_2 = nn.Linear(width,width)\n",
    "        self.layer_3 = nn.Linear(width,width)\n",
    "        self.layer_4 = nn.Linear(width,width)\n",
    "        self.layer_5 = nn.Linear(width,width)\n",
    "        self.layer_6 = nn.Linear(width,width)\n",
    "        self.layer_out = nn.Linear(width,output_size)\n",
    "    def forward(self,x):\n",
    "        output = self.layer_in(transform(x)) # transform for periodic\n",
    "        output = output + activation(self.layer_2(activation(self.layer_1(output)))) # residual block 1\n",
    "        output = output + activation(self.layer_4(activation(self.layer_3(output)))) # residual block 2\n",
    "        output = output + activation(self.layer_6(activation(self.layer_5(output)))) # residual block 3\n",
    "        output = self.layer_out(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 16 # dimension of input\n",
    "input_size = d * 2\n",
    "width_1 = 32\n",
    "width_2 = 32\n",
    "output_size_1 = 1\n",
    "output_size_2 = d\n",
    "data_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA = torch.cuda.is_available()\n",
    "# print('CUDA is: ', CUDA)\n",
    "if CUDA:\n",
    "    net_1 = Net(input_size, width_1, output_size_1).cuda() # network for u on gpu\n",
    "    net_2 = Net(input_size, width_2, output_size_2).cuda() # network for grad u on gpu\n",
    "else:\n",
    "    net_1 = Net(input_size, width_1, output_size_1) # network for u on cpu\n",
    "    net_2 = Net(input_size, width_2, output_size_2) # network for grad u on cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:0\" )\n",
    "# net_1.to(device)\n",
    "# net_2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(data_size_temp):\n",
    "    sample_temp = 2.0 * torch.rand(data_size_temp, d) - 1.0\n",
    "    return sample_temp.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_l2_error():\n",
    "    data_size_temp = 500\n",
    "    x = generate_sample(data_size_temp).cuda() \n",
    "    predict = net_1(x)\n",
    "    exact = u_ex(x)\n",
    "    value = torch.sqrt(torch.sum((predict - exact)**2))/torch.sqrt(torch.sum((exact)**2))\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xavier normal initialization for weights:\n",
    "#             mean = 0 std = gain * sqrt(2 / fan_in + fan_out)\n",
    "# zero initialization for biases\n",
    "def initialize_weights(self):\n",
    "    for m in self.modules():\n",
    "        if isinstance(m,nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight.data)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_weights(net_1)\n",
    "initialize_weights(net_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of net_1 and net_2\n",
    "param_num_1 = sum(x.numel() for x in net_1.parameters())\n",
    "param_num_2 = sum(x.numel() for x in net_2.parameters())\n",
    "# print(param_num_1)\n",
    "# print(param_num_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(x):\n",
    "#     x = generate_sample(data_size).cuda()\n",
    "#     x.requires_grad = True\n",
    "    u_hat = net_1(x)\n",
    "    grad_u_hat = torch.autograd.grad(outputs = u_hat, inputs = x, grad_outputs = torch.ones(u_hat.shape).cuda(), create_graph = True)\n",
    "    p_hat = net_2(x)\n",
    "    part_1 = torch.sum((grad_u_hat[0] - p_hat)**2) / len(x)\n",
    "    laplace_u = torch.zeros([len(p_hat), 1]).cuda()\n",
    "    for index in range(d):\n",
    "        p_temp = p_hat[:, index].reshape([len(p_hat), 1])\n",
    "        temp = torch.autograd.grad(outputs = p_temp, inputs = x, grad_outputs = torch.ones(p_temp.shape).cuda(), create_graph = True, allow_unused = True)[0]\n",
    "        laplace_u = temp[:, index].reshape([len(p_hat), 1]) + laplace_u\n",
    "    part_2 = torch.sum((-laplace_u + pi**2 * u_hat - f(x))**2)  / len(x)\n",
    "    return part_1 + part_2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam([\n",
    "                {'params': net_1.parameters()},\n",
    "                {'params': net_2.parameters()},\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 80000\n",
    "loss_record = np.zeros(epoch)\n",
    "error_record = np.zeros(epoch)\n",
    "time_start = time.time()\n",
    "for i in range(epoch):\n",
    "    optimizer.zero_grad()\n",
    "    x = generate_sample(data_size).cuda()\n",
    "    x.requires_grad = True\n",
    "    loss = loss_function(x)\n",
    "    loss_record[i] = float(loss)\n",
    "    error = relative_l2_error()\n",
    "    error_record[i] = float(error)\n",
    "    if i % 50 == 0:\n",
    "        print(\"current epoch is: \", i)\n",
    "        print(\"current loss is: \", loss.detach())\n",
    "        print(\"current error is: \", error.detach())\n",
    "    loss.backward()\n",
    "    optimizer.step() \n",
    "time_end = time.time()\n",
    "print('total time is: ', time_end-time_start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"loss_periodic_16d.npy\", loss_record)\n",
    "np.save(\"error_periodic_16d.npy\", error_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
